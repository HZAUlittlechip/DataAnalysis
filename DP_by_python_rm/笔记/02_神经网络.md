# 神经网络

Q：感知机的局限性在哪？神经网络的重要性质是什么？

感知机在选择合适的输入和权重的时候，还是需要人为的进行操作的。而神经网络是可以自动的从数据中来学习到合适的权重参数。

## 从感知机到神经网络

Q：什么是激活函数？它的作用是什么？激活的过程是如何实现的呢？

这里引入一个 **新的函数 h(x)**，拿之前感知机函数举例，其控制函数是 $y=b+w_1x_1~+w_2x_2$ 的形式，现在改为 $y = h(b+w_1x_1+w_2x_2)$，它将输入信号的总和转为输出信号，**这个 $h()$就是激活函数**(activation function)。

而 **它的作用** 是在于决定如何来激活信号的总合。其中在 $y = h(b+w_1x_1+w_2x_2）$ 中的 $b+w_1x_1+w_2x_2$ 可以合并为一个量，其为 **输入加权输入信号和偏置的总和** 视为节点 a。

<img src="./神经网络.assets/image-20250120231612892.png" alt="image-20250120231612892" style="zoom: 80%;" />

- “朴实感知机” - 单层网络；“多层感知机” - 神经网络

## 激活函数

激活函数的作用主要是给神经网络引入非线性

### 阶跃函数的实现

Q：什么是“阶跃函数”，如何实现？能想象出它的函数图是怎么样的吗？

函数公式为$\begin{align*}
h(x) = 
\begin{cases}
1 & (x > 0) \\
0 & (x \leq 0)
\end{cases}
\end{align*}$

阶跃函数的实现如下：

```python
import numpy as np
import matplotlib.pyplot as plt

# - 简单呈现形式 但其无法输入 np 的数组形式
def step_function_origin(x):
    if x > 0:
        return 1
    else:
        return 0
    
# - 可对 np 数组函数进行处理
def step_function(x):
    y = x > 0
    return y.astype(np.int32) # 元素类型转化
# 解释
x = np.array([-1.0, 1.0, 2.0])
y = x > 0 # array([False,  True,  True], dtype = bool)
y.astype(np.int32) # array([0, 1, 1])

# - 阶级函数图的实现
x = np.arange(-5.0, 5.0, 0.1) # 均匀步长矩阵
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```

### sigmoid 函数

$$
\begin{align*}
h(x) = \frac{1}{1 + \exp(-x)}
\end{align*}
$$

sigmoid 函数的范围在（0，1）

Q：sigmoid 函数和阶跃函数的联系和区别？

sigmoid 和阶跃函数都是 **非线性函数**

他们的区别在于：“平滑性”不同，阶跃函数是折线型的，而 sigmoid 函数是相对平滑的，**这种平滑性对神经网络的学习具有重要意义。**

<img src="./神经网络.assets/image-20250120232611565.png" alt="image-20250120232611565" style="zoom: 80%;" />

Q：利用 sigmoid 的神经网络和朴素感知机区别在哪？

朴实感知机神经元间流动的是 0 或 1 的二元信号，而神经网络则流动的是 **实数值信号**（平滑的 sigmoid）

Q：为什么神经网络一定要使用非线性函数呢？

采用非线性函数的话，多层或者说更加深层的神经元就没有意义了，无法发挥多层网络带来的优势。比如说激活函数 $h(x)=cx$，3 层神经网络的即 $y=h(h(h(x)))$，则其运算的过程是 $y(x)=c*c*c*x$ 和 $y(x)=a*x(a=c^3)$ 等效。因此要发挥出叠加层的优势，则必须要用到非线性函数。

sigmoid 函数的实现

```python
import numpy as np
def sigmoid(x)
	return 1 / (1 + np.exp(-x))
```



### ReLU 函数

Q：什么是 ReLU 函数

ReLU（Rectified Linear Unit）- 修正线性单元
$$
\begin{align*}
h(x) = 
\begin{cases}
x & (x > 0) \\
0 & (x \leq 0)
\end{cases}
\end{align*}
$$
当输入不超过 0 时输出 0，当输入超过 0 是时间则输出其本身，其函数可以简化为 $h(x)=max(0,x)$。其实现如下：

```python
def relu(x):
    return np.maximum(0, x)
```

## 多维数组的运算

### 神经网络的内积

下面是一个简单神经网络的矩阵运算，不含偏置 b 和激活函数，只运用的权重

<img src="./神经网络.assets/image-20250121211354273.png" alt="image-20250121211354273" style="zoom: 80%;" />

```python
import numpy as np
X = np.array([1, 2])
W = np.array([[1, 3, 5], [2, 4, 6]])
Y = np.dot(X, W) # [ 5 11 17]
```

## 3 层神经网络的实现

一个 3 层神经网络图简图

<img src="./神经网络.assets/image-20250121212658818.png" alt="image-20250121212658818" style="zoom: 80%;" />

上面的神经网络简单赋予输入值和权重可把前向传播的矩阵写为：
$$
\begin{align*}
\begin{pmatrix}
1 & 2
\end{pmatrix}_{1 \times 2} 
\begin{pmatrix}
1 & 3 & 5 \\
2 & 4 & 6
\end{pmatrix}_{3 \times 3} 
\begin{pmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{pmatrix}_{3 \times 3} 
\begin{pmatrix}
1 & 3 \\
2 & 4
\end{pmatrix}_{2 \times 2} =
\begin{pmatrix}
y1&y2
\end{pmatrix}
\end{align*}
$$

### 各层神经元间的传递

Q：神经元间的传递应该是这样的呢？如何用 numpy 的矩阵形式表示，大致的步骤有哪些？

本书的权重表示形式为：（和常规的矩阵表示方式不太相同）

<img src="./神经网络.assets/image-20250121222655080.png" alt="image-20250121222655080" style="zoom: 80%;" />

我们来看一下如何用矩阵的形式来表示带有偏置 b 的信号传递过程，传递的过程如图所示：

<img src="./神经网络.assets/image-20250121222929956.png" alt="image-20250121222929956" style="zoom: 80%;" />

a 还是我们之前看到的在基于权重 w 和偏差 b 的总和，因此整个到第一层的矩阵运算过程是：
$$
\begin{align*}
A^{(1)} = XW^{(1)} + B^{(1)}
\end{align*}
$$
其中，参数的具体形式为：

$$
\mathbf{A}^{(1)} = \begin{pmatrix} a_1^{(1)} & a_2^{(1)} & a_3^{(1)} \end{pmatrix}, \quad
\mathbf{X} = \begin{pmatrix} x_1 & x_2 \end{pmatrix}, \quad
\mathbf{B}^{(1)} = \begin{pmatrix} b_1^{(1)} & b_2^{(1)} & b_3^{(1)} \end{pmatrix}
$$

$$
\mathbf{W}^{(1)} =
\begin{pmatrix}
w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}
$$
> 可以注意到 $W^{(1)}$ 里面的矩阵格式和常规的矩阵的下标不一样，我认为还是以列来看这个矩阵.$w_{11}$ 的下标第一个 $1$ 是下一层的第 1 个节点, 第 2 个 $1$ 则是说明的是上一层的第 1 个节点.OK, 我们来看看 $W^{(1)}$ 矩阵, 第 1 列展现的是前向节点对后向第 1 个节点带来的贡献, 那么我们从第 1 行来看呢, 就可以看到的是前向的第 1 个节点对后向节点的贡献, 可以想象到从前向的第 1 个节点向后向的几个节点伸出 3 个小手来帮助他们吗？

至此, 我们来实现下这个矩阵吧:
```python
import numpy as np
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5],
              [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

A = np.dot(X, W1) + B1
print(A) # [0.3 0.7 1.1]
```

 当然我们还需要用激活函数 sigmoid 来处理一下 $A^{(1)}$，就成为了第 1 层的最终输出结果 $Z^{(1)}$，也是第 2 层的前向输入层

```python
Z1 = sigmoid(A) # array([0.57444252, 0.66818777, 0.75026011])
```

Q：最后一层（输出层）的激活函数是否和前面的也相同呢？ 

当然是不同的，输出层会根据其模型需要的功能的不同，而制定出不同的激活函数，而且为了和前面的激活函数 $h(x)$ 做出区分，输出层的激活函数被称为 $\sigma(x)$。

Q：输出层的函数如果根据解决问题的不同，可以分为几种？

- 分类问题
    - 二元分类问题：sigmoid 函数
    - 多分类问题：softmax 函数

- 回归问题：恒等函数

好的，那我们就此来实现一下这 3 层的神经网络吧：

1. `init_network()`: 对模型在传播过程中的偏置和权重进行初始化，并将其值记录在字典 network 中
2. `forward()`: 封装了从输入信号到输出信号的过程

```python
def init_network():
    ''' 网络结构为 1*2 -> 1*3 -> 1*2 -> 1*2 '''
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5],
                              [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4],
                              [0.2, 0.5],
                              [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3],
                              [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])

    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    # A = XW + B
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W1) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W1) + b3
    y = a3

    return y
```

## 输出层的设计

主要讲了恒等函数和 softmax 函数，将来如何实现 softmax 函数，还有它的一些改良措施（因为会出现溢值的情况），改量的最后结果为下，也讲了 softmax 函数的一些性质和理解，在分类的情况目标结果一般只输出最大的值，而 softmax 函数不会改变值的大小顺序，所有在实际的情况下不会用，且实际运算的 $exp{(a_i)}$ 还需要计算时间实际就更不会用了。【笔记丢失了，不想完全补了，就简单写点吧】

softmax 函数实现代码如下

```python
import numpy as np

def softmax(x):
    max_x = np.max(x)
    exp_new_x = np.exp(x - max_x)
    sum_exp_x = np.sum(exp_new_x)
    y = exp_new_x / sum_exp_x
    
	return y
```

对于2维度数组的计算为

```python
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T

    x = x - np.max(x) # 溢出对策
    return np.exp(x) / np.sum(np.exp(x))
```



## MNIST数据集预处理和批处理

看了这么多的内容，我们来开始尝试来上手一个数据集来尝试一下前向传播过程

> MNIST手写数据集是亚马逊发布的

  开始数据集的导入

```python
import pickle
import sys, os
from dataset.mnist import load_mnist # dataset是书作者自己写的掉数据集包
sys.path.append(os.pardir)

(x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten=True, normalize=False) 
	# 展开为1维数组，且讲像素保留为原来的0 ~ 255

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000,)
print(x_test.shape) # (10000, 784)
print(t_test.shape) # (10000,)
```

如何想看看训练标签和训练图像数据长什么样子，我们可以这样

```python
form PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img)) 
    # 从矩阵转化为矩阵，外加限定np格式
    
print(t_train[0]) # 5
img = img.reshape(28, 28) # 恢复拉升前的图像
img_show(img)
```

现在我们来正式来看看来构建这个神经网络

```python
from common.functions import sigmoid, softmax

def get_data():
    (x_train, t_train), (x_test, t_test) = \
    	load_mnist(normalize=True, flattern=True, one_hot_label=False)
    # 需要正则化不然sigmoid函数计算会溢出
    
    return x_test, t_test

def init_networl():
    with open("sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)  # 防止溢出用的第二种sigmoid
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)

    return y
```

`sample_weight.pkl`是预先读好的权重和偏置参数的字典文件，可以直接读取

`load_mnist(normalize, flattern, one_hot_lable)`其实默认输出为（训练集，训练验证集），（测试集，测试验证集）

- `normalize`默认为非正则化，正则化可以减少计算量,这种行为你也叫为预处理。
- `flattern`是图像压缩，T可以压缩为1维向量，
- `one_hot_label`是one-hot标签模式，T的情况就会讲验证集的数据正确的标签下标下的值变为1，其他标签下的值为0。

好的，我们来看看利用已有的网络参数能怎么看到模型的精确度，

```python
x, t = get_data()
network = init_network()

accuacy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
	p = np.argmax(y) # 输出预测最大可能性的标签
    if p == t[i]:
        accuracy_cnt += 1
        
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

### 批处理

批处理打包进行矩阵前向传播的计算，是可以加快计算速度的，对上面的循环计算过程进行改进一下，用上切片功能

```python
for i in range(0, len(x), batch_size=100)
	x_batch = x[i:i+batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis=1) # 沿第一轴的方向提取
    accuacy_cnt += np.sum(p == t[i:i+batch_size])
    
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

`np.argmax(y_batch, axis=1)`是从y_batch矩阵中沿第一轴的方向来提取最大值元素的下标，我们来见你简单的演示一下

```python
 x = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6],
               [0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])
    
y = np.argmax(x, axis=1) # 生成1维向量
print(y) # [1, 2, 1, 0]
```


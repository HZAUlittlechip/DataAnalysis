# 神经网络的学习

Q：什么是学习？学习的目的是什么？

学习：可以理解为从训练数据中自动获取最优权重和偏差的过程。

学习的目的：以损失函数为目标，要通过学习找出能让损失函数最小的权重和偏差参数。

## 从数据中学习

数据可以认为是机器学习的核心内容，常规的模型是通过人为的经验和直觉来判断出事物的规律性，机器学习则极力的去避免人的参与（但是在过程中还是有人为设定的部分），而神经网络或者说深度学习则进一步的去避免了人为的介入。

Q：来聊聊看机器学习中什么情况有人为设计的部分？

就拿手写数字识别来看，利用机器学习的技术的话，我们就需要从图像中提取数据的 **特征量**（就比如说像素的转化和放缩过程就是 **特征提取** 的过程-PRML），然后在通过机器学习的模型（比如是说 SVM 或 KNN）来进行学习，这个图像特征量的转化过程（将图像转为向量）其实就是有人为设计的过程，而对于不同的问题就需要设计不同的特征量。而在 **深度学习中这种重要特征量的过程也是由机器来完成的。**

> 深度学习我们也会称为其为端到端的学习（end-to-end machine learning），即从输入到输出，原始数据到目标结果。

Q：神经网络的优点是什么？

优点在于其可以用同样的一套方法来解决类似的一系列问题。

Q：机器学习中追求的是什么？

我们追求的是模型的 **泛化能力**，何为泛化能力呢？就是指处理未观察（非训练集）的数据能力，因此就有了训练数据集和测试数据集（以训练数据集来说其还包含了 **输入数据和目标数据（target data）**），泛化能力是机器学习的最终目的。

当然还要追求处理函数过拟合的问题。过拟合就是表现出单一数据集过渡拟合的状态，即不具有普适性当然也缺乏了泛化能力。

## 损失函数

我们可以把损失函数当成一个评价我们模型参数好坏具体的“幸福指数”。

Q：我们常把哪些函数当为损失函数呢？

- 均方差误差（mean squared error）
- 交叉熵误差(cross entropy error)

现在我们来简单的来介绍一下这两个误差函数

### 均方误差

$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$

$y$ 是神经网络的输出，$t$ 的监督数据，而 $k$ 是数据的维数，代码实现如下：

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

小插曲，我们来介绍一下 **one-hot 表示**

以手写识别里面的监督数据 $t$ 来演示的话，$ t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]$， 这样一个向量，可以表示为标签为”2“。解读一下就是，正确的标签对应下标的位置就设为 1，其他的设为 0，这个 $t$ 所代表的正确标签是 2，所有在 $t$ 下标 2 的位置就是 1

下面是一个简单的损失函数计算案例

```python
import numpy as np
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = mean_squared_error(y, t)
# 如何数组不是np数组要相应进行转化
# result = mean_squared_error(np.arrty(y), np.arrat(t)
print(result) # 0.09750000000000003
```

### 交叉熵误差

$$
E = - \sum_{k} t_k \log y_k
$$

顾名思义，据说交叉相乘而来，我个人认为乘法很适合 one-hot 表示，因为其不是正确的下标的元素都是 0。代码实现如下：

```python
def cross_entropy_error(y, t):
	delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

可以看到加了个 $delta$，这是因为当 $y$ 为 0 的时候，$log$ 函数会到负无穷，为了避免这种情况出现，就做出了这个保护性政策。

再来一个案例来看看针对上面的例子，通过交叉熵误差能得到什么样的损失函数值

```python
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = cross_entropy_error(y, t)
print(result) # 0.510825457099338
```

替换一下 $y$ 下标 3 和下标的 7 的值看看

```python
y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = cross_entropy_error(y, t)
print(result) # 2.302584092994546
```

损失函数的值增大了，因为输出 $y$ 对应下标的与监督向量显示对的值不一致，这是合理的

### mini-batch 学习

我们上面提到的均方差误差和交叉熵误差都是通过一个训练数据也可以说一个样本来展示的，但是肯定我们的数据集不只有一个样本，而针对更多的样本，我们就需要进行更多的上序计算，这样大大的增加的了计算量，以交叉熵为例子，其总的计算的函数为：
$$
E = - \frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk}
$$
计算大量的数据样本的损失函数是不现实的，所有我们从整个的数据样本中抽取一些样本来做”近似”，在神经网络的训练中，我们称之为 mini-batch（小批量）学习。

我们来演示一下这种小批量抽取的过程吧（比如从 60000 个训练样本中抽 10 个）：

```python
train_size = 60000 # MNIST手写数据集的样本数
batch_size = 10
batch_mask = np.random.choice(60000, 10) # 从60000个里面随机挑选10个生成一个数组
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

以上我们就完成了随机抽样的过程主要在于 `np.random.choice()` 这个函数

> 小科普：我们常听到的收视率也是随机抽取的家庭来代表的

### mini-batch 版交叉熵的实现

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size) # 转为2维向量
        y = y.reshape(1, y.size)
	
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

如果 $t$ 为非 one-hot 标签形式，则进行如下修改，可以只提取出对应标签下的 y 的概率（精准定位性提取），这和 one-hot 是一个意思，因为上面 one-hot 代码只有正确标签的概率才被提取出来了。好的，我们来看一下实现代码吧(可能需要理解一下，不行问 gpt)

```python
def cross_entropy_error(y, t): # t非ont-hot
    if y.ndim == 1:
        t = t.reshape(1, t.size) # 转为2维向量
        y = y.reshape(1, y.size)

	batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7)) / batch_size
```

### 为什么要设定损失函数

这是个非常好的问题，其实我们最终的目的不是为了模型有更加的精度吗？直接关注精度不就好啦？这是不行的。来解释一下吧

我们要得到好的精度当然要对模型的参数进行调整，但是调整的话，调整的依据是什么呢？是精度还是损失函数？其实是根据他们的导数来判断了，应该说是梯度，通过导数我们可以知道损失函数或者说精度是要变大还是变小。那为什么不拿精度来判断模型是否调整好呢？是因为如果以识别精度为指标的话，参数的导数在绝大多少地方都会变成 0。比如说哈，我们拿出 100 个训练样本进行训练，识别正确了 30 个，精度就为 30%，为了提高精度我们就要微调一下模型的参数，但调整后我们能精度可能不会变化，因为不一定就会多识别正确一个，可以看出精度的变化是离散的，所有就谈不上什么导数和梯度了。这时就要用到损失函数，随着模型参数的改变损失函数是一直发生变化的，根据其变化修正参数后，模型的梯度也是会发生变化的，这样就可以让我们进一步来调整，主要我们要体现出这个 **连续性**

当然如果我们激活函数是阶跃函数，我们的学习任务也无法进行下去（大部分位置导数为 0），损失函数变化的意义也就是失去了，所有我们常用 sigmoid 函数来当激活函数，就在于其平滑的连续性。

## 数值微分

### 导数

$$
\frac{d f(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

我们很清楚导数求解的解析式形式，那我们直接就写出其 python 代码吧

> numerical differentiation ：数值微分
>
> 数值微分：用数值的方式近视求解函数的过程 or 用微小的差分求导数的过程

```python
def numerical_diff(f, x):
    h = 1e-30
    return (f(x + h) - f(x)) / h
```

Q：看的出来其这个函数的缺点吗？

1. 存在 **舍入误差（rounding error**），`h = 1e-30` h 的值太小了，为提高效率就会节省精细部分的小数内容，导致输出的之值为 0，建议改为 ` h = 1e-4` 就可以达到计算的精度了
2. 函数表达式的差分处理有问题，$f(x+h)-f(x)$  这种形式的差分产生的切线和“真实”的切线还是不相同的，要进行改进，建议采用 **中心差分** $f(x+h)-f(x-h)$，这样计算就是以 $x$ 为中心点了，提高的精度

修改后的代码如下：

```python
def numercial_diff(f, x):
    h = 1e-4
    return (f(x + h) - f(x - h)) / (2 * h) # 主要分子成为2h
```

### 偏导数

来举一个简单的例子吧，以下面的多元函数为例，其函数代码实现如下（有两个变量哦）：
$$
f(x_0, x_1) = x_0^2 + x_1^2
$$

```python
def function_2(x):
    return np.sum(x ** 2)
	# or return x[0] ** 2 + x[1] ** 2
```

我们来看看 $ x_0=3$, $x_1=4$ 时的 $x_0$ 偏导数如何计算吧，这是我们认为 $x_1$ 部分的内容就是常数了, 看上有些原始和愚笨吧，后面会介绍直接全部计算的函数的

```python
def function_tmp1(x0): # 展示的函数表达
    return x0 * x0 + 4.0**2.0 

numerical_diff(function_tmpl, 3.0) # 调用前面的导数求解函数
# 结果为 6.00000000000378
```

## 梯度

梯度的定义可以简单的解释为：全部变量偏导而汇集而成的向量。可以这样实现：

1. 针对 1 维数组 x（变量）的情况

```python
def numerical_gradient_1(f, x):
    """ 输入 处理函数f 和 输入量x 输出梯度"""
    h = 1e-4
    grad = np.zeros_like(x) # 生成和x形状相同的数组，数组内都是0

    for idx in range(x.size):
        tmp_val = x[idx] # 中间变量
        x[idx] = tmp_val + h # f(x + h)计算
        f_xh1 = f(x)

        x[idx] = tmp_val - h # f(x - h）计算
        f_xh2 = f(x)

        grad[idx] = (f_xh1 - f_xh2)/(h * 2)
        x[idx] = tmp_val # 还原x

    return grad
```

2. 针对 2 维数组 x（变量）的情况

```python
def numerical_gradient_2(f, x):
    h = 1e-4  # 偏移量
    grad = np.zeros_like(x)  # 初始化梯度数组

    # 针对二维数组的每个元素计算梯度
    for i in range(x.shape[0]):  # 遍历行
        for j in range(x.shape[1]):  # 遍历列
            tmp_val = x[i, j]

            # 计算f(x+h) 和 f(x-h)
            x[i, j] = tmp_val + h
            fxh1 = f(x)

            x[i, j] = tmp_val - h
            fxh2 = f(x)

            # 数值梯度
            grad[i, j] = (fxh1 - fxh2) / (2 * h)
            x[i, j] = tmp_val  # 恢复原始值

    return grad
```

### 梯度法

**梯度表示是各点处的函数值减少最多的方向**。这是一个很重要的性质，请一定要记住。

> 但是其梯度的方向未必就是最小值的方向，也不一定是正在应该前进的方向。

> 当函数很复杂且扁平的时候，梯度法的学习过程中可能会进入一个（几乎）平坦的地区，陷入“学习高原”的无法前进的停滞期，我们称之为”梯度消失“

我们在来回顾一下我们为什么要了解梯度，机器学习的过程就是在学习中寻找到最优参数。神经网络的构建当然也要去找到最优的参数（权重的偏置）。而这里的最优参数，是损失函数求到最小时的参数，我们就利用梯度这个判断工具来寻找这个损害函数的最小值，也在这个过程中去寻找最优的参数。

简单的介绍一下 **梯度法（gradient method）** 吧（或者说是梯度下降法）：

函数的取值从当前位置沿着梯度方向前进一段举例，然后在新的地方重新来求取梯度，再沿新梯度的方向来前进，如此反复不断的沿梯度方向前进，逐渐减少函数值的过程。

我们来用数学的语言来表示一个梯度计算的中间过程吧，
$$
x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}
$$

$$
x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}
$$

可以看出在偏导前面有个 $\eta$，这个 $\eta$ 代表的是学习率（learning rate）。其表示了在一次学习中，模型应该学习多少，多大程度上的更新参数。这个值过大或过小都无法达到满意的效果。学习率过大，最终结果会发散到一个很大的值；而学习率过小的话，基本模型没怎么更新就结束了。我们来看一下代码如何来实现这个更新的过程

> 学习率这样的参数称为超参数，它是需要人工设定的

> 常见超参数还有：批量大小（batch size）；正则化参数（regularzation paremeter）：控制模型复杂度，防止过拟合；迭代次数（Epochs）；隐藏层的数量和大小；激活函数等

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    x = init_x

    for i in range(step_num):
        grad = numerical_gradient_1(f, x)
        x -= lr * grad

    return x
```

### 神经网络的梯度

很显然我们我们之前计算的都是针对向量的，神经网络中的输入值和权重的计算是矩阵间的计算。我们也来实现一下吧，先来构建一个简单的神经网络。

```python
# 简单神经网络的构建
""" 需要函数:softmax, cross_entropy_error, numerical_gradient """
class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3) # 随机用高斯分布生成权重矩阵

    def predict(self, x):
        return np.dot(x, self.W)

    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)

        return loss
```

`loss(self, x, t)` 输出的是计算的损失函数，其实计算中也内含到了权重 $W$（在 `predict` 过程中），所有说 loss 函数也是关于 $W$ 的函数也没有问题，我们可以这样重新定义一下这个函数 $f()$ 为 $f(W)$，我们来举一个实例来看吧

```python
net = simpleNet()
# 随便输入的
x = np.array([0.6, 0.7])
t = np.arrry([0, 0, 1])

def f(W):
    return net.loss(x, t)
# 也可以这么些
f = lambda W: net.loss(x, t)
```

我们接下来就来计算一下它的梯度，用到我们上面函数 `numerical_grdaient_2(f, x)`

```python
dW = numerical_gradient_2(f, net.W) # 用针对2维矩阵的
```

求完梯度我们就只需在根据梯度法来更新权重就好，就是我们学习的过程了。

## 学习算法的实现

上个章节我们基本实现了前向传播的实例，我们现在来看看，增加了梯度下降方法调整模型参数的学习过程，有点小复杂，但是都是前面学习的内容，不要太紧张

这里小提一嘴，我们本次定义的网络模型中，用到的梯度计算方式是只用到了上面的数值微分梯度计算方法，计算效率有点低，如果需要更快的计算速度，需要用到后面的反向误差传播法的计算方法

我们这里来定义一个 `TwoLayerNet` 的类在存储我们的模型函数，

```python
import numpy as np
import os, sys
sys.path.append(os.pardir)
from DP_by_python_rm.code.common.functions import *
from dataset.mnist import load_mnist

class TwoLayerNet:
    """ """
    def __init__(self, input_size, hidden_size, output_size,
                 weight_init_std=0.01):
        self.params = { }
        self.params['W1'] = weight_init_std * \
            np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * \
            np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']

        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        z2 = sigmoid(a2)
        y = softmax(z2)

        return y

    def loss(self, x, t):
        y = self.predict(x)

        return cross_entropy_error_onehot(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        """ 较为低效后续会替换为误差反向传播法
        """
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient_2d(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient_2d(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient_2d(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient_2d(loss_W, self.params['b2'])

        return grads
```

我们来看看整个类是什么作用

- 整个构造函数的作用是去权重和偏置参数的初始化：` def __init__(self, input_size, hidden_size, output_size,weight_init_std=0.01):` 可以看到这类需要我们输入：输入层的神经元数、隐藏层的个数、输出层的个数
    - `weight_init_std=0.01` 是为了约束初始权重的标准差，因为我们初始权重是随机生成的，如果权重初始化不当，可能会导致梯度消失或梯度爆炸，或者训练过程过慢，甚至无法收敛。

- 值的预测 `predict(self, x)`，利用输入的数据，通过网络，得到最后的预测结果 $y$

- 损失函数 `loss(self, x, t)`， 利用交叉熵误差的方法来计算得到损失函数
- 准确率计算 `accuracy(self, x, t)`， 计算模型识别的准确率。最后的计算公式我们可以来研究一下 `accuracy = np.sum(y == t) / float(x.shape[0])` 十分的简洁
    - `x.shape[0]` ：返回的是样本的个数
    - `len(x[0])`：返回的是样本第一个向量的长度，比如在这个 MNIST 数据集里面就是 784

- 数值梯度计算 `numerical_gradient(self, x, t)`，计算经过前向传播后的数值梯度，为后续的模型参数的更新提供帮助，它的功能只计算梯度且讲梯度传递到字典中存储，具体的梯度处理过程还没出现

我们来个例子来演示一下，

```python
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, one_hot_label=True)


# 超参数设置
iters_num = 10000 # 梯度法的循环次数
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1


train_loss_list = []
train_acc_list = []
test_acc_list = []
# 平均每个epoch的重复次数
iter_per_epoch = max(train_size / batch_size, 1)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):
    # 获取mini——batch
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 梯度计算
    grad = network.numerical_gradient(x_batch, t_batch) \
        # 这个梯度计算的方法确实巨慢

    # 更新参数
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    # 计算每个epoch的识别精度
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(f"train acc, test acc |  {str(train_acc)},\
        {str(test_acc)}")
```

本次实践添加了控制梯度迭代次数的 `iters_num = 10000`，同时也引入了 **$epoch$** 的总结方法，它是一个单位，一个 epoch 表示学习中所有所有训练数据都被训练一次的更新次数，对于我们 MNIST 训练集有 60000 笔训练数据，我们的 mini-batch 的间隔是 100，那么一个 epoch 就是 $60000/100 = 600$ 次更新

同时我们可以发现前面计算的精度只是针对一个 mini-batch 的，不具有普适性也就是不具有泛化能力，所有我们针对每个 epoch 都会再集中起来计算一次对训练集和测试集和精度，这样即不会很大的计算量也可以直观的看到精度的具体变化

到了迭代计算的时间了，我们可以看到 `batch_mask` 的产生是通过正态随机生成的，其实常规的 mini-batch 提取不是这样的，常规的手段是：先将数据打乱，然后按照指定的批次来根据随机后的序号来选取 mini-batch，这样每个 mini-batch 按照分的顺序就可以各安排一个索引号，然后再根据这个索引号就可以遍历所有 mini-batch，且遍历完就遍历完所有的数据了，这就被称为一个 epoch。但是本文介绍的方式没有这样而都是随机生成的，所有不是所有的数据都会被随机到。

这个更新参数的函数也是值得注意的，可以记忆一下，通过读取字典的键来进行计算，同时可以再反过来更新参数

```python
for key in ('W1', 'b1', 'W2', 'b2')
	network.params[key] -= learning_rate * grad[key]
```












































# 神经网络的学习

Q：什么是学习？学习的目的是什么？

学习：可以理解为从训练数据中自动获取最优权重和偏差的过程。

学习的目的：以损失函数为目标，要通过学习找出能让损失函数最小的权重和偏差参数。

## 从数据中学习

数据可以认为是机器学习的核心内容，常规的模型是通过人为的经验和直觉来判断出事物的规律性，机器学习则极力的去避免人的参与（但是在过程中还是有人为设定的部分），而神经网络或者说深度学习则进一步的去避免了人为的介入。

Q：来聊聊看机器学习中什么情况有人为设计的部分？

就拿手写数字识别来看，利用机器学习的技术的话，我们就需要从图像中提取数据的 **特征量**（就比如说像素的转化和放缩过程就是 **特征提取** 的过程-PRML），然后在通过机器学习的模型（比如是说 SVM 或 KNN）来进行学习，这个图像特征量的转化过程（将图像转为向量）其实就是有人为设计的过程，而对于不同的问题就需要设计不同的特征量。而在 **深度学习中这种重要特征量的过程也是由机器来完成的。**

> 深度学习我们也会称为其为端到端的学习（end-to-end machine learning），即从输入到输出，原始数据到目标结果。

Q：神经网络的优点是什么？

优点在于其可以用同样的一套方法来解决类似的一系列问题。

Q：机器学习中追求的是什么？

我们追求的是模型的 **泛化能力**，何为泛化能力呢？就是指处理未观察（非训练集）的数据能力，因此就有了训练数据集和测试数据集（以训练数据集来说其还包含了 **输入数据和目标数据（target data）**），泛化能力是机器学习的最终目的。

当然还要追求处理函数过拟合的问题。过拟合就是表现出单一数据集过渡拟合的状态，即不具有普适性当然也缺乏了泛化能力。

## 损失函数

我们可以把损失函数当成一个评价我们模型参数好坏具体的“幸福指数”。

Q：我们常把哪些函数当为损失函数呢？

- 均方差误差（mean squared error）
- 交叉熵误差(cross entropy error)

现在我们来简单的来介绍一下这两个误差函数

### 均方误差

$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$

$y$ 是神经网络的输出，$t$ 的监督数据，而 $k$ 是数据的维数，代码实现如下：

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

小插曲，我们来介绍一下 **one-hot 表示**

以手写识别里面的监督数据 $t$ 来演示的话，$ t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]$， 这样一个向量，可以表示为标签为”2“。解读一下就是，正确的标签对应下标的位置就设为 1，其他的设为 0，这个 $t$ 所代表的正确标签是 2，所有在 $t$ 下标 2 的位置就是 1

下面是一个简单的损失函数计算案例

```python
import numpy as np
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = mean_squared_error(y, t)
# 如何数组不是np数组要相应进行转化
# result = mean_squared_error(np.arrty(y), np.arrat(t)
print(result) # 0.09750000000000003
```

### 交叉熵误差

$$
E = - \sum_{k} t_k \log y_k
$$

顾名思义，据说交叉相乘而来，我个人认为乘法很适合one-hot表示，因为其不是正确的下标的元素都是0。代码实现如下：

```python
def cross_entropy_error(y, t):
	delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

可以看到加了个$delta$，这是因为当$y$为0的时候，$log$函数会到负无穷，为了避免这种情况出现，就做出了这个保护性政策。

再来一个案例来看看针对上面的例子，通过交叉熵误差能得到什么样的损失函数值

```python
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = cross_entropy_error(y, t)
print(result) # 0.510825457099338
```

替换一下$y$下标3和下标的7的值看看

```python
y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = cross_entropy_error(y, t)
print(result) # 2.302584092994546
```

损失函数的值增大了，因为输出$y$对应下标的与监督向量显示对的值不一致，这是合理的

### mini-batch学习

我们上面提到的均方差误差和交叉熵误差都是通过一个训练数据也可以说一个样本来展示的，但是肯定我们的数据集不只有一个样本，而针对更多的样本，我们就需要进行更多的上序计算，这样大大的增加的了计算量，以交叉熵为例子，其总的计算的函数为：
$$
E = - \frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk}
$$
计算大量的数据样本的损失函数是不现实的，所有我们从整个的数据样本中抽取一些样本来做”近似”，在神经网络的训练中，我们称之为mini-batch（小批量）学习。

我们来演示一下这种小批量抽取的过程吧（比如从60000个训练样本中抽10个）：

```python
train_size = 60000 # MNIST手写数据集的样本数
batch_size = 10
batch_mask = np.random.choice(60000, 10) # 从60000个里面随机挑选10个生成一个数组
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

以上我们就完成了随机抽样的过程主要在于`np.random.choice()`这个函数

> 小科普：我们常听到的收视率也是随机抽取的家庭来代表的

### mini-batch版交叉熵的实现

明在弄












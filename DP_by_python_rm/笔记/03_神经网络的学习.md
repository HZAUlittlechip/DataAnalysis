# 神经网络的学习

Q：什么是学习？学习的目的是什么？

学习：可以理解为从训练数据中自动获取最优权重和偏差的过程。

学习的目的：以损失函数为目标，要通过学习找出能让损失函数最小的权重和偏差参数。

## 从数据中学习

数据可以认为是机器学习的核心内容，常规的模型是通过人为的经验和直觉来判断出事物的规律性，机器学习则极力的去避免人的参与（但是在过程中还是有人为设定的部分），而神经网络或者说深度学习则进一步的去避免了人为的介入。

Q：来聊聊看机器学习中什么情况有人为设计的部分？

就拿手写数字识别来看，利用机器学习的技术的话，我们就需要从图像中提取数据的 **特征量**（就比如说像素的转化和放缩过程就是 **特征提取** 的过程-PRML），然后在通过机器学习的模型（比如是说 SVM 或 KNN）来进行学习，这个图像特征量的转化过程（将图像转为向量）其实就是有人为设计的过程，而对于不同的问题就需要设计不同的特征量。而在 **深度学习中这种重要特征量的过程也是由机器来完成的。**

> 深度学习我们也会称为其为端到端的学习（end-to-end machine learning），即从输入到输出，原始数据到目标结果。

Q：神经网络的优点是什么？

优点在于其可以用同样的一套方法来解决类似的一系列问题。

Q：机器学习中追求的是什么？

我们追求的是模型的 **泛化能力**，何为泛化能力呢？就是指处理未观察（非训练集）的数据能力，因此就有了训练数据集和测试数据集（以训练数据集来说其还包含了 **输入数据和目标数据（target data）**），泛化能力是机器学习的最终目的。

当然还要追求处理函数过拟合的问题。过拟合就是表现出单一数据集过渡拟合的状态，即不具有普适性当然也缺乏了泛化能力。

## 损失函数

我们可以把损失函数当成一个评价我们模型参数好坏具体的“幸福指数”。

Q：我们常把哪些函数当为损失函数呢？

- 均方差误差（mean squared error）
- 交叉熵误差(cross entropy error)

现在我们来简单的来介绍一下这两个误差函数

### 均方误差

$$
E = \frac{1}{2} \sum_{k} (y_k - t_k)^2
$$

$y$ 是神经网络的输出，$t$ 的监督数据，而 $k$ 是数据的维数，代码实现如下：

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t)**2)
```

小插曲，我们来介绍一下 **one-hot 表示**

以手写识别里面的监督数据 $t$ 来演示的话，$ t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]$， 这样一个向量，可以表示为标签为”2“。解读一下就是，正确的标签对应下标的位置就设为 1，其他的设为 0，这个 $t$ 所代表的正确标签是 2，所有在 $t$ 下标 2 的位置就是 1

下面是一个简单的损失函数计算案例

```python
import numpy as np
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = mean_squared_error(y, t)
# 如何数组不是np数组要相应进行转化
# result = mean_squared_error(np.arrty(y), np.arrat(t)
print(result) # 0.09750000000000003
```

### 交叉熵误差

$$
E = - \sum_{k} t_k \log y_k
$$

顾名思义，据说交叉相乘而来，我个人认为乘法很适合 one-hot 表示，因为其不是正确的下标的元素都是 0。代码实现如下：

```python
def cross_entropy_error(y, t):
	delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

可以看到加了个 $delta$，这是因为当 $y$ 为 0 的时候，$log$ 函数会到负无穷，为了避免这种情况出现，就做出了这个保护性政策。

再来一个案例来看看针对上面的例子，通过交叉熵误差能得到什么样的损失函数值

```python
y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = cross_entropy_error(y, t)
print(result) # 0.510825457099338
```

替换一下 $y$ 下标 3 和下标的 7 的值看看

```python
y = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])
t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
result = cross_entropy_error(y, t)
print(result) # 2.302584092994546
```

损失函数的值增大了，因为输出 $y$ 对应下标的与监督向量显示对的值不一致，这是合理的

### mini-batch 学习

我们上面提到的均方差误差和交叉熵误差都是通过一个训练数据也可以说一个样本来展示的，但是肯定我们的数据集不只有一个样本，而针对更多的样本，我们就需要进行更多的上序计算，这样大大的增加的了计算量，以交叉熵为例子，其总的计算的函数为：
$$
E = - \frac{1}{N} \sum_{n} \sum_{k} t_{nk} \log y_{nk}
$$
计算大量的数据样本的损失函数是不现实的，所有我们从整个的数据样本中抽取一些样本来做”近似”，在神经网络的训练中，我们称之为 mini-batch（小批量）学习。

我们来演示一下这种小批量抽取的过程吧（比如从 60000 个训练样本中抽 10 个）：

```python
train_size = 60000 # MNIST手写数据集的样本数
batch_size = 10
batch_mask = np.random.choice(60000, 10) # 从60000个里面随机挑选10个生成一个数组
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

以上我们就完成了随机抽样的过程主要在于 `np.random.choice()` 这个函数

> 小科普：我们常听到的收视率也是随机抽取的家庭来代表的

### mini-batch 版交叉熵的实现

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size) # 转为2维向量
        y = y.reshape(1, y.size)
	
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

如果 $t$ 为非 one-hot 标签形式，则进行如下修改，可以只提取出对应标签下的 y 的概率（精准定位性提取），这和 one-hot 是一个意思，因为上面 one-hot 代码只有正确标签的概率才被提取出来了。好的，我们来看一下实现代码吧(可能需要理解一下，不行问 gpt)

```python
def cross_entropy_error(y, t): # t非ont-hot
    if y.ndim == 1:
        t = t.reshape(1, t.size) # 转为2维向量
        y = y.reshape(1, y.size)

	batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size),y] + 1e-7)) / batch_size
```

### 为什么要设定损失函数

这是个非常好的问题，其实我们最终的目的不是为了模型有更加的精度吗？直接关注精度不就好啦？这是不行的。来解释一下吧

我们要得到好的精度当然要对模型的参数进行调整，但是调整的话，调整的依据是什么呢？是精度还是损失函数？其实是根据他们的导数来判断了，应该说是梯度，通过导数我们可以知道损失函数或者说精度是要变大还是变小。那为什么不拿精度来判断模型是否调整好呢？是因为如果以识别精度为指标的话，参数的导数在绝大多少地方都会变成 0。比如说哈，我们拿出 100 个训练样本进行训练，识别正确了 30 个，精度就为 30%，为了提高精度我们就要微调一下模型的参数，但调整后我们能精度可能不会变化，因为不一定就会多识别正确一个，可以看出精度的变化是离散的，所有就谈不上什么导数和梯度了。这时就要用到损失函数，随着模型参数的改变损失函数是一直发生变化的，根据其变化修正参数后，模型的梯度也是会发生变化的，这样就可以让我们进一步来调整，主要我们要体现出这个 **连续性**

当然如果我们激活函数是阶跃函数，我们的学习任务也无法进行下去（大部分位置导数为 0），损失函数变化的意义也就是失去了，所有我们常用 sigmoid 函数来当激活函数，就在于其平滑的连续性。

## 数值微分

### 导数

$$
\frac{d f(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$

我们很清楚导数求解的解析式形式，那我们直接就写出其 python 代码吧

> numerical differentiation ：数值微分
>
> 数值微分：用数值的方式近视求解函数的过程 or 用微小的差分求导数的过程

```python
def numerical_diff(f, x):
    h = 1e-30
    return (f(x + h) - f(x)) / h
```

Q：看的出来其这个函数的缺点吗？

1. 存在 **舍入误差（rounding error**），`h = 1e-30` h 的值太小了，为提高效率就会节省精细部分的小数内容，导致输出的之值为 0，建议改为 ` h = 1e-4` 就可以达到计算的精度了
2. 函数表达式的差分处理有问题，$f(x+h)-f(x)$  这种形式的差分产生的切线和“真实”的切线还是不相同的，要进行改进，建议采用 **中心差分** $f(x+h)-f(x-h)$，这样计算就是以 $x$ 为中心点了，提高的精度

修改后的代码如下：

```python
def numercial_diff(f, x):
    h = 1e-4
    return (f(x + h) - f(x - h)) / (2 * h) # 主要分子成为2h
```

### 偏导数

来举一个简单的例子吧，以下面的多元函数为例，其函数代码实现如下（有两个变量哦）：
$$
f(x_0, x_1) = x_0^2 + x_1^2
$$

```python
def function_2(x):
    return np.sum(x ** 2)
	# or return x[0] ** 2 + x[1] ** 2
```

我们来看看$ x_0=3$,$x_1=4$ 时的$x_0$偏导数如何计算吧，这是我们认为$x_1$部分的内容就是常数了,看上有些原始和愚笨吧，后面会介绍直接全部计算的函数的

```python
def function_tmp1(x0): # 展示的函数表达
    return x0 * x0 + 4.0**2.0 

numerical_diff(function_tmpl, 3.0) # 调用前面的导数求解函数
# 结果为 6.00000000000378
```

## 梯度





